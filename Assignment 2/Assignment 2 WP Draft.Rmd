---
title: "Assignment 2"
author:
- Gao Haochun A0194525Y
- Ge Siqi A0194550A
- Wang Pei A0194486M
- Wei Yifei A0203451W
date: "3/4/2021"
output: pdf_document
---

```{r wd}
setwd("/Users/wangpei/OneDrive - National University of Singapore/Curriculum/Sem_04/BT3102/Assignments/Computational-Methods-for-Business-Analytics/Assignment 2/")
getwd()
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(stargazer)
library(AER)
```

# Q1. You study how sales depend on prices for wine. You believe that rating (i.e., expert ratings) can be an imperfect measure of taste (i.e., true quality). Taste is unobserved because there is no ideal measure for it.

## I. Assume that causal Diagram 1 is correct. Choose sensible parameter values and simulate a data set of N = 10000 observations for 3 variables: ratings, prices, and sales (taste data is removed after the simulation because it is unobserved to the analyst).
```{r q1.1}
set.seed(37)
N = 10000
# taste in (0, 1), 2 decimal places
taste = sample(seq(0,1,0.01), N, replace=T)
hist(taste)

trans = taste*3 + 7 + rnorm(N)
# map to (0, 10) and round to 1 decimal place
rating = round(10*(trans - min(trans))/(max(trans)-min(trans)), 1)
hist(rating)

price = 10*taste + 1000 + rnorm(N, 0, 50)
hist(price)

sales = 20000 + 2000*taste - 10*price + rnorm(N, 0, 50)
hist(sales)
```

## II. Use the data set you just generated and regress sales on price. How does your estimate for the price coefficient differ from its true value? Does including ratings as an independent variable solve the problem? Explain why or why not.
```{r q1.2 regression}
model1 = lm(sales ~ price)
summary(model1)

```

## III. Redo I-II and this time assume that causal Diagram 2 is correct.
```{r q1.3.1}

```

```{r q1.3.2}

```


# Q2
## 2.I
Data generating process: $\alpha_1=0,\alpha_2=\alpha_3=\beta_1=\beta_2=\beta_3=\gamma=1$

```{r Q2.1 DGP}
set.seed(37)
N = 10000

D = rnorm(N)
E = rnorm(N)
F = rnorm(N)
a1=0
a2=a3=b1=b2=b3=g=1

C = g*F + rnorm(N)
A = 0 + a2*C + a3*D + rnorm(N)
B = b1*A + b2*C + b3*E + rnorm(N)
```

### 2.I.1 draw causal diagram

### 2.I.2 Show all collider variables and how they may bias estimates.

Collider variables are variables with multiple parents.
A is a collider variable, it will cause endogenous problem if added into regressions of D and C (i.e. D ~ C+A, C ~ D+A yield biased estimates.)

B is a collider variable, it will cause endogenous problem if added into regressions of A, C, E. (i.e. C ~ E+B, E ~ C+B, A ~ C+B, C ~ A+B, A ~ E+B, E ~ A+B, etc.)

### 2.I.3 Which variables to include to predict A? Is the model also a good causal inference model?

Regress A on D and C (A ~ D+C). It is also a good model for causal inference as it captures the true causal relationship (No endogenous problem).

### 2.I.4 Show whether or not each of following data is enough to identify relation between A and B.
```{r Q2.1.4}
set.seed(37)
C_measured = C + rnorm(N)
D_measured = D + rnorm(N)
A_measured = A + rnorm(N)

lm1 = lm(B ~ A+C) # ok
lm2 = ivreg(B ~ A|D) # ok
lm3 = lm(B ~ A+E) # bad, omitted variable bias by C
lm4 = lm(B ~ A+F) # bad, omitted variable bias by C
lm5 = lm(B ~ A+C_measured) # ok, only coeff of C is biased
lm6 = ivreg(B ~ A|D_measured) # bad, D is weak iv
lm7 = ivreg(B ~ A_measured|D) # ok
lm8 = lm(B ~ A_measured+C) # bad, measurement error by A

stargazer(lm1,lm2,lm3,lm4,lm5,lm6,lm7,lm8, type="text",omit.stat=c("LL","ser","f"),
          model.numbers=TRUE, model.names = TRUE)
```
Comments on models:
a: B ~ A+C can identify relation between A and B because E is exogenous and C, which is endogenous, is included in the regressoin.

b: B ~ A|D can identify relation between A and B because D is a good instrumental variable as it strongly correlates with A and it correlates with B only through A. 

c: B ~ A+E cannot identify relation between A and B due to omitted variable bias. C effects both A and B and is omitted in the regression.

d: Both (B ~ A+F) and (B ~ A|F) cannot identify relation between A and B because C is omitted in the regression, causing omitted variable bias. F cannot be a instrumental variable as it correlates with B not only through A.

e: B ~ A + C_measured can identify relation between A and B (but bot B and C). This is because:
$$
B = b_0 + b_1 A + b_2 C + \epsilon_1,\ \  \\
C^* = C + \epsilon_2 \\
\implies B = b_0 + b_1 A + b_2 C^* + (\epsilon_1 - b_2 \epsilon_2)
$$
The error term of B ~ A+C* is correlated C*, but not A. Thus, the estimation for b2 is biased but the estimation for b1 is unbiased.

f: B ~ A|D_measured cannot identify the relation between A and B because D_measured is not a good instrumental variable. As shown below, the R-squared between A and D_measured is merely 0.1284, and the regression coefficient is not significant, so D_measured is a bad instrumental variable for A.
```{r A and D_measured}
summary(lm(A ~ D_measured))
```

g: B ~ A_measured|D can identify the relation between A and B because D as instrumental variable can fix the attenuation effect of measurement error on A.

h: B ~ A_measured + C cannot identify the relation between A and B because the error term of the regression is correlated with A_measured, causing biased estimation of b1.


### I.2 
```{r Q2.2}
set.seed(37)
N = 10000

D = rnorm(N)
E = rnorm(N)
F = rnorm(N)
a1=a2=a3 = -0.8
b1=b2=b3 = -0.5
g = 0.5
e1 = rnorm(N)
e2 = rnorm(N)
e3 = rnorm(N)

C = g*F + e3
B = (e2 + b3*E + a3*b1*D + (a2*b1+b2)*C)/(1-a1*b1)
A = e1 + a1*B + a2*C + a3*D

# increase D by 1
D2 = D+1
B2 = (e2 + b3*E + a3*b1*D2 + (a2*b1+b2)*C)/(1-a1*b1)
A2 = e1 + a1*B2 + a2*C + a3*D2
A2[1] - A[1] # Decrease by 4/3

# increase E by 1
E2 = E+1
B3 = (e2 + b3*E2 + a3*b1*D + (a2*b1+b2)*C)/(1-a1*b1)
A3 = e1 + a1*B3 + a2*C + a3*D
A3[1] - A[1] # increase by 2/3

# increase F by 1
F2 = F+1
C2 = g*F2 + e3
B4 = (e2 + b3*E + a3*b1*D + (a2*b1+b2)*C2)/(1-a1*b1)
A4 = e1 + a1*B4 + a2*C2 + a3*D
A4[1] - A[1] # decrease by 1/3


```



### I.3
```{r Q2.3}
library(stargazer)
library(AER)
data = read.csv("hw2q2.csv")
attach(data)
g_lm = lm(C ~ F) # ok
a3_lm = lm(A ~ D) # bad
b3_lm = lm(B ~ E) # bad
a1_a2_a3_lm = lm(A ~ B+C+D) # ok, ???????????????
a1_a2_lm = ivreg(A ~ B+C|E+F) # ok
b1_b2_b3_lm = lm(B ~ A+C+E) # bad
b1_b2_lm = ivreg(B ~ A+C|D+F) # ok

stargazer(a3_lm,g_lm,b3_lm,a1_a2_a3_lm,a1_a2_lm,b1_b2_b3_lm, b1_b2_lm,type="text",omit.stat=c("LL","ser","f"),
          model.numbers=TRUE, model.names = TRUE)
```



## II

### II.1



### II.2

**II.2.a**: When D increases by 1, A will ___.

**II.2.b**: When E increases by 1, A will ___.

**II.2.c**: When F increases by 1, A will ___.


### II.3



## III

# Q3

## I
```{r q3}
library(stargazer)
library(AER)
data = read.csv("Attend.csv")
data$fresh = as.factor(data$fresh)
data$soph = as.factor(data$soph)
attach(data)

lm1 = lm(stndfnl ~ atndrte+fresh+soph)
summary(lm1)

# Not so confident? May have confounding vars.
```
### I.1


### I.2


## II
```{r q2}
lm2 = lm(stndfnl ~ atndrte+fresh+soph+priGPA+ACT)
summary(lm2)
```
### II.1

### II.2

## III

```{r q3.3}
lm3 = ivreg(stndfnl ~ atndrte + fresh + soph + priGPA + ACT|hwrte)

stargazer(lm1, lm2, lm3, type="text",omit.stat=c("LL","ser","f"),
          model.numbers=TRUE, model.names = TRUE)
```
