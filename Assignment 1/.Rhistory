x = rnorm(N)
e = runif(N)
Y = 0.7 + 2*x + e
X = cbind(rep(1,N),x)
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)
res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.999372
mean(res2) # 2.000016
var(res1) # 0.003772373
var(res2) # 0.0008630136
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.3446
#Anderson-Darling normality test
ad.test(res1) # p-value = 0.4207
qqnorm(res1)
qqline(res1)
ad.test(res1) # p-value = 0.4207
qqnorm(res2)
qqline(res2)
set.seed(37)
S = 10000
single_loop = function(N) {
x = rnorm(N)
e = rcauchy(N)
Y = 0.7 + 2*x + e
X = cbind(rep(1,N),x)
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)
res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.907382x
mean(res2) # -1.599096
var(res1) # 3427.213
var(res2) # 29851.75
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.05461
#Anderson-Darling normality test
ad.test(res1) # p-value = 2.2e-16
qqnorm(res1)
qqline(res1)
ad.test(res1) # p-value = 2.2e-16
qqnorm(res2)
qqline(res2)
library(numDeriv)
obj_func = function(r, x0=10000,p=250, n=60) {
# Iteration method to calculate xn
x = x0
for(i in 1:n) {
x = x*(1+r) - p
}
res = x
}
analytical_func = function(r, x0=10000,p=250, n=60) {
# Close form of xn
xn = x0*(1+r)^n + p*(1-(1+r)^n)/r
}
bisection_method = function(f,left,right,tol=1e-10,n=1000) {
if(sign(f(left))==sign(f(right))) {
print("Bad Initial Points!")
stop()
}
history = right
for (i in 1:n) {
mid = (left+right)/2
if (f(mid)==0) {
history = c(history,mid)
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
} else if (sign(f(mid))==sign(f(right))) {
right = mid
history = c(history,right)
} else {
left = mid
history = c(history,left)
}
if(abs(left-right) < tol) {
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
}
}
print("Maximum number of iteration reached")
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
}
newton_method = function(f, r0, tol=1e-8,n=1000) {
history = r0
for(i in 1:n) {
deriv = genD(func = f, x = r0)$D
r1 = r0 - f(r0)/deriv[1]
history = c(history,r1)
if(abs(r1-r0) < tol) {
res = list('optim r' = r1, 'iterations' = i,'history'=history)
return(res)
}
r0 = r1
}
print("Maximum number of iteration reached")
res = list('optim r' = r1, 'iterations' = i,'history'=history)
return(res)
}
sol1 = newton_method(obj_func, 0.01)
sol1$`optim r`
sol2 = newton_method(analytical_func, 0.02)
sol3 = newton_method(analytical_func, 0.03)
sol4 = bisection_method(obj_func, 0.01, 0.025)
sol5 = bisection_method(analytical_func, 0.01, 0.015)
mat = cbind(sol1$history,sol2$history,sol3$history)
maxLen = nrow(mat)
plot(1:length(sol5$history),sol5$history,type="b",pch=20,col="blue",ylim=c(0.01,0.03),xlab="iterations",ylab="r")
abline(sol1$`optim r`,0,col="red")
lines(1:length(sol2$history),sol2$history,type="b",pch=20,col="red")
lines(1:length(sol1$history),sol1$history,type="b",pch=20,col="green")
lines(1:length(sol4$history),sol4$history,type="b",pch=20,col="purple")
lines(1:length(sol3$history),sol3$history,type="b",pch=20,col="orange")
set.seed(37)
invSampling = function(N) {
# Returns a vector of N elements sampled by inversion.
return(runif(N)^(1/3))
}
hist(invSampling(10^6))
set.seed(37)
revenue = function(x,y) {
return(y*(log(x)+1)-y^2*sqrt(1-x^4))
}
X = invSampling(1000000)
expectedReturn = function(y) {
# Compute expected return given y
N = 1000000
#X = invSampling(N)
Y = rep(y,N)
# Take mean of 10^6 sampled return to approximate expectation
res = sum(mapply(revenue, X, Y))/N
}
# take y from 0 to 1 with step 0.01
y = seq(0,1,0.01)
returns = sapply(y,expectedReturn)
plot(returns ~ y, type="l")
abline(0,0)
plot(returns ~ y, type="l")
abline(0,0)
optim(0, function(y){-1*expectedReturn(y)})
#0.4639308
knitr::opts_chunk$set(echo = TRUE)
newton_optim = function(f, x0, tol=1e-8, n=1000, maximize=FALSE) {
if(maximize) {
f = function(x) {-1*f(x)}
}
history = x0
for(i in 1:n) {
deriv = genD(func=f, x=x0)$D
x1 = x0 - deriv[1]/deriv[2]
history = c(history, x1)
if(abs(x1 - x0) < tol) {
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# Continue iteration
x0 = x1
}
print("Maximum number of iteration reached")
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# optim(0, function(y){-1*expectedReturn(y)})
newton_optim(expectedReturn, 0.01)
knitr::opts_chunk$set(echo = TRUE)
library(Matrix) # Matrix operations
library(MASS) # For Moore-Penrose pseudo-inverse ginv()
rankMatrix(matrix(c(1,1,2,2),nrow=2))
rankMatrix(matrix(c(1,1,2,2,1,2),nrow=3))
rankMatrix(matrix(c(1,1,2,0),nrow=2))
rankMatrix(matrix(c(1,2,3,2,0,2),nrow=3))
df = read.csv("exercise_and_cholesterol.csv")
old_df = df[df$Age=="Old",]
young_df = df[df$Age=="Young",]
attach(df)
plot(Cholesterol ~ Exercise,col=ifelse(Age=="Old","red","blue"),
pch=ifelse(Age=="Old",20,17),ylim=c(35,50),xlim=c(6,18))
all_model = lm(Cholesterol ~ Exercise, data=df)
young_model = lm(Cholesterol ~ Exercise, data=young_df)
old_model = lm(Cholesterol ~ Exercise, data=old_df)
abline(all_model$coefficients[1],all_model$coefficients[2], lty="solid")
abline(young_model$coefficients[1],young_model$coefficients[2], lty="dotted",col="blue")
abline(old_model$coefficients[1],old_model$coefficients[2], lty="dashed",col="red")
legend(6, 50, legend=c("Old only", "Young only","Combined data"),
col=c("red", "blue","black"), lty=c("dotted","dashed","solid"), cex=1)
plot(young_df$Cholesterol ~ young_df$Exercise,col="blue",pch=17,ylim=c(35,50),xlim=c(6,18),xlab="",ylab="")
par(new=TRUE)
plot(old_df$Cholesterol ~ old_df$Exercise,col="red",pch=20,ylim=c(35,50),xlim=c(6,18),xlab="Exercise",ylab="Cholesterol")
all_model = lm(Cholesterol ~ Exercise + Age, data=df)
young_model = lm(Cholesterol ~ Exercise, data=young_df)
old_model = lm(Cholesterol ~ Exercise, data=old_df)
abline(all_model$coefficients[1],all_model$coefficients[2], lty="solid")
abline(young_model$coefficients[1],young_model$coefficients[2], lty="dotted",col="blue")
abline(old_model$coefficients[1],old_model$coefficients[2], lty="dashed",col="red")
legend(6, 50, legend=c("Old only", "Young only","Combined data"),
col=c("red", "blue","black"), lty=c("dotted","dashed","solid"), cex=1)
set.seed(37)
# Data generation
x = rnorm(1000) # Sample 1000 points from N(0, 1)
e = rnorm(1000)
y = 0.7 + 2*x + e
plot(y ~ x)
# Use lm()
model = lm(y ~ x)
summary(model)
# y = 0.734 + 1.954 * x
# Use matrix algebra
X = cbind(rep(1,1000),x) # Add bias(constant for intercept) to data matrix
Y = y
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta_h # \beta0 = 0.734, \beta1 = 1.954
plot(y ~ x)
abline(model$coefficients[1],model$coefficients[2],col="green")
abline(beta_h[1],beta_h[2],col="red",lty="dashed")
legend(-3,8,legend=c("lm()", "Matrix algebra"),col=c("green","red"),lty=c("dashed","dashed"))
X = cbind(rep(1,1000), x)
Y = y
# Objective function
fun = function(beta) {
sum((Y - X%*%beta)^2)
}
# Start optimization with random initialization
optim(runif(2),fun) # \beta0 = 0.734, \beta1 = 1.954
beta1 = cov(x,y)/var(x)
beta0 = mean(y) - beta1*mean(x)
beta0 # 0.734
beta1 # 1.954
set.seed(37)
S = 10000
single_loop = function(N) {
x = rnorm(N)
e = rnorm(N)
Y = 0.7 + 2*x + e
X = cbind(rep(1,N),x)
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)
res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 2.002271
mean(res2) # 1.998513
var(res1) # 0.04465469
var(res2) # 0.01046778
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.1095
# Test normality
library(nortest)
#Anderson-Darling normality test
ad.test(res1) # p-value = 2.064e-08
qqnorm(res1)
qqline(res1)
ad.test(res1) # p-value = 2.064e-08
qqnorm(res2)
qqline(res2)
set.seed(37)
S = 10000
single_loop = function(N) {
x = rnorm(N)
e = runif(N)
Y = 0.7 + 2*x + e
X = cbind(rep(1,N),x)
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)
res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.999372
mean(res2) # 2.000016
var(res1) # 0.003772373
var(res2) # 0.0008630136
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.3446
#Anderson-Darling normality test
ad.test(res1) # p-value = 0.4207
qqnorm(res1)
qqline(res1)
ad.test(res1) # p-value = 0.4207
qqnorm(res2)
qqline(res2)
set.seed(37)
S = 10000
single_loop = function(N) {
x = rnorm(N)
e = rcauchy(N)
Y = 0.7 + 2*x + e
X = cbind(rep(1,N),x)
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)
res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.907382x
mean(res2) # -1.599096
var(res1) # 3427.213
var(res2) # 29851.75
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.05461
#Anderson-Darling normality test
ad.test(res1) # p-value = 2.2e-16
qqnorm(res1)
qqline(res1)
ad.test(res1) # p-value = 2.2e-16
qqnorm(res2)
qqline(res2)
library(numDeriv)
obj_func = function(r, x0=10000,p=250, n=60) {
# Iteration method to calculate xn
x = x0
for(i in 1:n) {
x = x*(1+r) - p
}
res = x
}
analytical_func = function(r, x0=10000,p=250, n=60) {
# Close form of xn
xn = x0*(1+r)^n + p*(1-(1+r)^n)/r
}
bisection_method = function(f,left,right,tol=1e-10,n=1000) {
if(sign(f(left))==sign(f(right))) {
print("Bad Initial Points!")
stop()
}
history = right
for (i in 1:n) {
mid = (left+right)/2
if (f(mid)==0) {
history = c(history,mid)
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
} else if (sign(f(mid))==sign(f(right))) {
right = mid
history = c(history,right)
} else {
left = mid
history = c(history,left)
}
if(abs(left-right) < tol) {
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
}
}
print("Maximum number of iteration reached")
res = list('optim r' = mid, 'iterations' = i,'history'=history)
return(res)
}
newton_method = function(f, r0, tol=1e-8,n=1000) {
history = r0
for(i in 1:n) {
deriv = genD(func = f, x = r0)$D
r1 = r0 - f(r0)/deriv[1]
history = c(history,r1)
if(abs(r1-r0) < tol) {
res = list('optim r' = r1, 'iterations' = i,'history'=history)
return(res)
}
r0 = r1
}
print("Maximum number of iteration reached")
res = list('optim r' = r1, 'iterations' = i,'history'=history)
return(res)
}
sol1 = newton_method(obj_func, 0.01)
sol1$`optim r`
sol2 = newton_method(analytical_func, 0.02)
sol3 = newton_method(analytical_func, 0.03)
sol4 = bisection_method(obj_func, 0.01, 0.025)
sol5 = bisection_method(analytical_func, 0.01, 0.015)
mat = cbind(sol1$history,sol2$history,sol3$history)
maxLen = nrow(mat)
plot(1:length(sol5$history),sol5$history,type="b",pch=20,col="blue",ylim=c(0.01,0.03),xlab="iterations",ylab="r")
abline(sol1$`optim r`,0,col="red")
lines(1:length(sol2$history),sol2$history,type="b",pch=20,col="red")
lines(1:length(sol1$history),sol1$history,type="b",pch=20,col="green")
lines(1:length(sol4$history),sol4$history,type="b",pch=20,col="purple")
lines(1:length(sol3$history),sol3$history,type="b",pch=20,col="orange")
set.seed(37)
invSampling = function(N) {
# Returns a vector of N elements sampled by inversion.
return(runif(N)^(1/3))
}
hist(invSampling(10^6))
set.seed(37)
revenue = function(x,y) {
return(y*(log(x)+1)-y^2*sqrt(1-x^4))
}
X = invSampling(1000000)
expectedReturn = function(y) {
# Compute expected return given y
N = 1000000
#X = invSampling(N)
Y = rep(y,N)
# Take mean of 10^6 sampled return to approximate expectation
res = sum(mapply(revenue, X, Y))/N
}
# take y from 0 to 1 with step 0.01
y = seq(0,1,0.01)
#returns = sapply(y,expectedReturn)
plot(returns ~ y, type="l")
abline(0,0)
newton_optim = function(f, x0, tol=1e-8, n=1000, maximize=FALSE) {
if(maximize) {
f = function(x) {-1*f(x)}
}
history = x0
for(i in 1:n) {
deriv = genD(func=f, x=x0)$D
x1 = x0 - deriv[1]/deriv[2]
history = c(history, x1)
if(abs(x1 - x0) < tol) {
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# Continue iteration
x0 = x1
}
print("Maximum number of iteration reached")
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# optim(0, function(y){-1*expectedReturn(y)})
newton_optim(expectedReturn, 0.01)
#0.4639308
newton_optim = function(f, x0, tol=1e-8, n=1000, maximize=FALSE) {
if(maximize) {
f = function(x) {-1*f(x)}
}
history = x0
for(i in 1:n) {
deriv = genD(func=f, x=x0)$D
x1 = x0 - deriv[1]/deriv[2]
history = c(history, x1)
if(abs(x1 - x0) < tol) {
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# Continue iteration
x0 = x1
}
print("Maximum number of iteration reached")
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# optim(0, function(y){-1*expectedReturn(y)})
#0.4639308
newton_optim(expectedReturn, 0)
newton_optim = function(f, x0, tol=1e-10, n=1000, maximize=FALSE) {
if(maximize) {
f = function(x) {-1*f(x)}
}
history = x0
for(i in 1:n) {
deriv = genD(func=f, x=x0)$D
x1 = x0 - deriv[1]/deriv[2]
history = c(history, x1)
if(abs(x1 - x0) < tol) {
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# Continue iteration
x0 = x1
}
print("Maximum number of iteration reached")
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# optim(0, function(y){-1*expectedReturn(y)})
#0.4639308
newton_optim(expectedReturn, 0)
install.packages("TMB")
# optim(0, function(y){-1*expectedReturn(y)})
#0.4639308
#newton_optim(expectedReturn, 0)
library(TMB)
install.packages("TMB")
# optim(0, function(y){-1*expectedReturn(y)})
#0.4639308
#newton_optim(expectedReturn, 0)
library(TMB)
newton(0, function(y){-1*expectedReturn(y)})
optim(0, function(y){-1*expectedReturn(y)})
?Brent
??Brent
optimize(0, function(y){-1*expectedReturn(y)})
optimize(function(y){-1*expectedReturn(y)},c(0,1))
newton_optim = function(f, x0, tol=1e-10, n=1000, maximize=FALSE) {
if(maximize) {
f = function(x) {-1*f(x)}
}
history = x0
for(i in 1:n) {
deriv = genD(func=f, x=x0)$D
x1 = x0 - deriv[1]/deriv[2]
history = c(history, x1)
if(abs(x1 - x0) < tol) {
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
# Continue iteration
x0 = x1
}
print("Maximum number of iteration reached")
return(list(`optim x`=x1, `iterations`=i, `history`=history))
}
optimize(function(y){-1*expectedReturn(y)},c(0,1))
# 0.4637619
newton_optim(expectedReturn, 0)
