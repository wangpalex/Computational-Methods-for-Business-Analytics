---
title: "Assignment 1 Submission"
author:
  - Gao Haochun A0194525Y
  - Ge Siqi A0194550A
  - Wang Pei A0194486M
  - Wei Yifei A0203451W
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Q1

### I. Which of the 4 data violate(s) the rank condition in OLS regression?

$$\begin{equation*}
A = 
\begin{bmatrix}
1 & 1 \\
2 & 2
\end{bmatrix}

B = 
\begin{bmatrix}
1 & 1 \\
2 & 2 \\
1 & 2
\end{bmatrix}

C = 
\begin{bmatrix}
1 & 1 \\
2 & 0
\end{bmatrix}
D = 
\begin{bmatrix}
1 & 2 & 3 \\
2 & 0 & 2
\end{bmatrix}
\end{equation*}$$

The full assumption can be denoted by rank(X)=k, where X is a n*k matrix, n observations, k independent variables.Thus, full rank assumption can be interpreted as: all the columns are linearly independent. In other words, all independent variables (including the constant) are linearly independent.

```{r Q1-1}
library(Matrix) # Matrix operations
library(MASS) # For Moore-Penrose pseudo-inverse ginv()
A = matrix(c(1,1,2,2),nrow=2)
B = matrix(c(1,1,2,2,1,2),nrow=3)
C = matrix(c(1,1,2,0),nrow=2)
D = matrix(c(1,2,3,2,0,2),nrow=3)
rankMatrix(A)
rankMatrix(B)
rankMatrix(C)
rankMatrix(D)
```
A: This is a square matrix. The matrix A can be reduced to
$$A' = 
\begin{bmatrix}
1 & 0 \\
1 & 0
\end{bmatrix}$$
The reduced form has one zero column. The two columns are linearly dependent and Rank(A)=1. Therefore, A is not full rank.

B: This is a non-square matrix. The matrix B can be reduced to
$$B' = 
\begin{bmatrix}
1 & 0 \\
2 & 0 \\
1 & 1
\end{bmatrix}$$
The reduced form does not have zero column. Therefore, two columns are linearly independent and Rank(B)=2. Thus, B is full rank.

C: This is a square matrix. The matrix C can be reduced to
$$C' = 
\begin{bmatrix}
1 & 0 \\
2 & -2
\end{bmatrix}$$
The reduced form does not have zero column. Therefore, two columns are linearly independent and Rank(C)=2. Thus, C is full rank.


D: This is a non-square matrix.. The matrix D can be reduced to
$$D' = 
\begin{bmatrix}
1 & 0 & 0 \\
2 & -4 & 0
\end{bmatrix}$$

The reduced form has one zero column. The two columns are linearly dependent and Rank(D)=2. Therefore, D is not full rank.


### II Suppose that you wanted to build a model of the approval ratings of major party nominees for US president. You included the following 4 independent variables: Years holding elected office; Party; Gender; Indicator variable for being married to a former president. Does this violate the rank condition? Explain.

Yes, this violates the rank condition.
The assumption of full rank condition is violated if one independent vairable is perfectly linearly dependent on another independent variable, or an independent variable is perfectly jointly determined by other independent varaibles. In this context, the indicator variable for being married to a former president is perfectly linearly dependent on the gender of the nominees. Based on the history of US presidents, all past presidents are male and there is no cases of a man marrying to male US past presidents. Therefore, if an nominee is male (Gender == Male), the indicator variable for being married to a former president can only be 0 (not married to a former president). In other words, the indicator variable for being married to a former president is perfectly dependent on the gender. If both "gender" and "being married to a former president" are included as independet variables in the matrix X, the column of “gender” and the column of “being married to a former president” will be linearly dependent. Since there is perfect linear dependence between independent variables, this violates the rank condition in OLS regression.


### III Give a different real-world example where the rank condition fails.

One example is to compare the weight of secondary school student between Cedar Girls' Secondary School (girls school) and Victoria School (boys school). In this case, the dependent variable is the weight of a student, and the independent variables include a dummy variable indicating the student's school, gender, age, height etc. The independent variable school is defined as school == 0 if the student is from Cedar Girls' Secondary School, school == 1 if the student is from Victoria School. The independent variable gender is defined as gender == 0 if the student is female, gender ==  1 if the student is male.

Because all the students from Cedar Girls' Secondary School are girls and all those from Victoria School are boys, the independent variable school is perfectly linearly dependent on another independent variable gender. If we include both "gender" and "school" as independet variables in the matrix X, the column of “school” and the column of “gender” will be linearly dependent. The assumption of full rank condition is violated if one independent vairable is perfectly linearly dependent on another independent variable. Therefore, the rank condition fails in this example.


## Q2

### I. Reproducing the plot
```{r Q2-1}
df = read.csv("exercise_and_cholesterol.csv")
old_df = df[df$Age=="Old",]
young_df = df[df$Age=="Young",]

attach(df)
plot(Cholesterol ~ Exercise,col=ifelse(Age=="Old","red","blue"),
     pch=ifelse(Age=="Old",20,17),ylim=c(35,50),xlim=c(6,18))

all_model = lm(Cholesterol ~ Exercise, data=df)
young_model = lm(Cholesterol ~ Exercise, data=young_df)
old_model = lm(Cholesterol ~ Exercise, data=old_df)
abline(all_model$coefficients[1],all_model$coefficients[2], lty="solid")
abline(young_model$coefficients[1],young_model$coefficients[2], lty="dotted",col="blue")
abline(old_model$coefficients[1],old_model$coefficients[2], lty="dashed",col="red")
legend(6, 50, legend=c("Old only", "Young only","Combined data"),
       col=c("red", "blue","black"), lty=c("dotted","dashed","solid"), cex=1)
```

### II. Are the three lines giving consistent or conflicting insights? Explain why.

Yes, the three lines are giving conflicting insights. Below are the insights from the three lines:
	- The regression line based on the combined data implies that there is a positive linear correlation between cholesterol and exercise.
	- The regression line based on the data from only young people implies that there is a negative linear correlation between  cholesterol and exercise.
	- The regression line based on the data from only old people implies that there is a negative linear correlation between  cholesterol and exercise.
	- Old people tend to have higher cholesterol level than the young. This can be implied from the regressin line of old only data being higher than the regression line of young only data.

The insights drawn from subgroups are different from that drawn from the combined data. This is because age is another factor affecting the level of cholesterol as well as the amount of exercise. To be sopecific, age is positively correlated with cholestrol level. The conflict in insights can be due to omitted variable bias in the model of Cholesterol~Exercise.



### III. Can you propose an alternative estimation using all the data in order to obtain the same insight as the two separate regressions using only the old or the young people?

An alternative estimation is to include age as an independent variable in the model and make it a dummy variable: age = 1 if one is young, age = 0 if one is old. Run a linear regression of cholesterol against age and exercise using all the data. This gives us three regression lines with similar trend as shown on the graph below. Now, the same insghts can be generated.

In addition, by running the linear regression using the model Cholesterol~Exercise + Age, the coefficient of the independent variable exercise is -0.9384, which suggests a negative correlation between exercise and cholesterol of a person i.e. the increase in the amount of exercise by 1 unit results in a decrease in the cholesterol level of that particular person by 0.9384 units. Also, the coefficient of the independent variable Age is -9.8511, which suggests that the young has a lower cholesterol level than old people by 9.8511 units in average. Then, this further proves that we obtain the same insight as the two separate regressions.
```{r Q2-3}
plot(young_df$Cholesterol ~ young_df$Exercise,col="blue",pch=17,ylim=c(35,50),xlim=c(6,18),xlab="",ylab="")
par(new=TRUE)
plot(old_df$Cholesterol ~ old_df$Exercise,col="red",pch=20,ylim=c(35,50),xlim=c(6,18),xlab="Exercise",ylab="Cholesterol")

all_model = lm(Cholesterol ~ Exercise + Age, data=df)
young_model = lm(Cholesterol ~ Exercise, data=young_df)
old_model = lm(Cholesterol ~ Exercise, data=old_df)
abline(all_model$coefficients[1],all_model$coefficients[2], lty="solid")
abline(young_model$coefficients[1],young_model$coefficients[2], lty="dotted",col="blue")
abline(old_model$coefficients[1],old_model$coefficients[2], lty="dashed",col="red")
legend(6, 50, legend=c("Old only", "Young only","Combined data"),
       col=c("red", "blue","black"), lty=c("dotted","dashed","solid"), cex=1)

```

## Q3

### Data generation
```{r Q3.1.a}
set.seed(37)
# Data generation
x = rnorm(1000) # Sample 1000 points from N(0, 1)
e = rnorm(1000)
y = 0.7 + 2*x + e
plot(y ~ x)
```
### Use R's lm() function: 
$$\hat{y} = 0.734 + 1.954 x$$
```{r Q3.1.b}
# Use lm()
model = lm(y ~ x)
summary(model)
# y = 0.734 + 1.954 * x
```
### Use matrix algebra: 
$$\hat{\beta} = (X^T X)^{-1} X^T Y \\
\implies
\hat{y} = 0.734 + 1.954 x$$

```{r Q3.1.c}
# Use matrix algebra
X = cbind(rep(1,1000),x) # Add bias(constant for intercept) to data matrix
Y = y
beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
beta_h # \beta0 = 0.734, \beta1 = 1.954

plot(y ~ x)
abline(model$coefficients[1],model$coefficients[2],col="green")
abline(beta_h[1],beta_h[2],col="red",lty="dashed")
legend(-3,8,legend=c("lm()", "Matrix algebra"),col=c("green","red"),lty=c("dashed","dashed"))
```
### Use R's optim(): 
$$\hat{\beta} = argmin_{\beta} \sum_{i=1,...,n}(y_i - \beta_0 - \beta_1 \times x_i)^2 \\
\implies
\hat{y} = 0.734 + 1.954 x$$

```{r Q3.1.d1}
X = cbind(rep(1,1000), x)
Y = y
# Objective function
fun = function(beta) {
  sum((Y - X%*%beta)^2)
}
# Start optimization with random initialization
optim(runif(2),fun) # \beta0 = 0.734, \beta1 = 1.954
```
### Use formula for 1 variable regression: 
$$\hat{\beta_1} = \frac{Cov(x_i,y_i)}{Var(x_i)} \\
\hat{\beta_0} = \bar{y} - \hat{\beta_1} \bar{x} \\
\implies
\hat{y} = 0.734 + 1.954 $$

```{r Q3.1.d2}
beta1 = cov(x,y)/var(x)
beta0 = mean(y) - beta1*mean(x)
beta0 # 0.734
beta1 # 1.954
```

Check distribution of $\hat{\beta_1}$
```{r Q3.2.1.a}
set.seed(37)
S = 10000
single_loop = function(N) {
  x = rnorm(N)
  e = rnorm(N)
  Y = 0.7 + 2*x + e
  X = cbind(rep(1,N),x)
  beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
  beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)


res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 2.002271
mean(res2) # 1.998513
var(res1) # 0.04465469
var(res2) # 0.01046778
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.1095

# Test normality
library(nortest)
#Anderson-Darling normality test
ad.test(res1) # p-value = 2.064e-08
qqnorm(res1)
qqline(res1)

ad.test(res1) # p-value = 2.064e-08
qqnorm(res2)
qqline(res2)
```

### Use uniformly distributed error
```{r Q3.2.2.b}
set.seed(37)
S = 10000
single_loop = function(N) {
  x = rnorm(N)
  e = runif(N)
  Y = 0.7 + 2*x + e
  X = cbind(rep(1,N),x)
  beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
  beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)

res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.999372
mean(res2) # 2.000016
var(res1) # 0.003772373
var(res2) # 0.0008630136
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.3446

#Anderson-Darling normality test
ad.test(res1) # p-value = 0.4207
qqnorm(res1)
qqline(res1)

ad.test(res1) # p-value = 0.4207
qqnorm(res2)
qqline(res2)
```

### Use Cauchy distributed error
```{r Q3.2.2.c}
set.seed(37)
S = 10000
single_loop = function(N) {
  x = rnorm(N)
  e = rcauchy(N)
  Y = 0.7 + 2*x + e
  X = cbind(rep(1,N),x)
  beta_h = ginv(t(X)%*%X)%*%t(X)%*%Y
  beta1 = beta_h[2]
}
vec_loop = Vectorize(single_loop)

res1 = vec_loop(rep(25,S))
hist(res1)
res2 = vec_loop(rep(100,S))
hist(res2)
mean(res1) # 1.907382x
mean(res2) # -1.599096
var(res1) # 3427.213
var(res2) # 29851.75
t.test(res1, res2, alternative = "two.sided", var.equal = FALSE) # p-value = 0.05461

#Anderson-Darling normality test
ad.test(res1) # p-value = 2.2e-16
qqnorm(res1)
qqline(res1)

ad.test(res1) # p-value = 2.2e-16
qqnorm(res2)
qqline(res2)
```

## Q4
Recurrent formula of compound interest rate:
$$x_{n+1} = x_n (1+r) - P \\
\implies
x_n = x_0 (1+r)^n + P \frac{1-(1+r)^n}{r}$$

Given: $$x_0 = 10000, x_{60} = 0, P = 250 \\
\implies
r = 0.01439478$$

```{r Numertical methods} 
library(numDeriv)

obj_func = function(r, x0=10000,p=250, n=60) {
  # Iteration method to calculate xn
  x = x0
  for(i in 1:n) {
    x = x*(1+r) - p
  }
  res = x
}

analytical_func = function(r, x0=10000,p=250, n=60) {
  # Close form of xn
  xn = x0*(1+r)^n + p*(1-(1+r)^n)/r
}

bisection_method = function(f,left,right,tol=1e-10,n=1000) {
  if(sign(f(left))==sign(f(right))) {
    print("Bad Initial Points!")
    stop()
  }
  
  history = right
  for (i in 1:n) {
    mid = (left+right)/2
    if (f(mid)==0) {
      history = c(history,mid)
      res = list('optim r' = mid, 'iterations' = i,'history'=history)
      return(res)
    } else if (sign(f(mid))==sign(f(right))) {
      right = mid
      history = c(history,right)
    } else {
      left = mid
      history = c(history,left)
    }
    
    if(abs(left-right) < tol) {
      res = list('optim r' = mid, 'iterations' = i,'history'=history)
      return(res)
    }
  }
  print("Maximum number of iteration reached")
  res = list('optim r' = mid, 'iterations' = i,'history'=history)
  return(res)
}

newton_method = function(f, r0, tol=1e-8,n=1000) {
  history = r0
  for(i in 1:n) {
    deriv = genD(func = f, x = r0)$D
    r1 = r0 - f(r0)/deriv[1]
    history = c(history,r1)
    if(abs(r1-r0) < tol) {
      res = list('optim r' = r1, 'iterations' = i,'history'=history)
      return(res)
    }
    r0 = r1
  }
  print("Maximum number of iteration reached")
  res = list('optim r' = r1, 'iterations' = i,'history'=history)
  return(res)
}

sol1 = newton_method(obj_func, 0.01)
sol1$`optim r`
sol2 = newton_method(analytical_func, 0.02)
sol3 = newton_method(analytical_func, 0.03)
sol4 = bisection_method(obj_func, 0.01, 0.025)
sol5 = bisection_method(analytical_func, 0.01, 0.015)

mat = cbind(sol1$history,sol2$history,sol3$history)
maxLen = nrow(mat)
plot(1:length(sol5$history),sol5$history,type="b",pch=20,col="blue",ylim=c(0.01,0.03),xlab="iterations",ylab="r")
abline(sol1$`optim r`,0,col="red")
lines(1:length(sol2$history),sol2$history,type="b",pch=20,col="red")
lines(1:length(sol1$history),sol1$history,type="b",pch=20,col="green")
lines(1:length(sol4$history),sol4$history,type="b",pch=20,col="purple")
lines(1:length(sol3$history),sol3$history,type="b",pch=20,col="orange")
```

## Q5
### I. Use inversion sampling to simulate 10. random draws for market condition x.

Find the inverse function of X
$$f(x)=3x^2 \cdot I(0,1) \\
\implies 
F(x) = \int_0^x f(x)dx = x^3,x \in (0,1) \\
\implies
x = F^{-1}(u) = u^{1/3}$$

```{r Q5-1}
set.seed(37)

invSampling = function(N) {
  # Returns a vector of N elements sampled by inversion.
  return(runif(N)^(1/3))
}

hist(invSampling(10^6))
```
## II. Calculate the expected returns using your draws. Plot the expected returns for y ∈
(0,1).

Expected return given y:
$$E[p(x,y)|y] = \int_0^1 p(x,y)f(x) dx$$

```{r Q5-2}
set.seed(37)

revenue = function(x,y) {
  return(y*(log(x)+1)-y^2*sqrt(1-x^4))
}

X = invSampling(1000000)
expectedReturn = function(y) {
  # Compute expected return given y
  N = 1000000
  #X = invSampling(N)
  Y = rep(y,N)
  
  # Take mean of 10^6 sampled return to approximate expectation
  res = sum(mapply(revenue, X, Y))/N
}
```

```{r Q5-2 run simulation}
# take y from 0 to 1 with step 0.01
y = seq(0,1,0.01)
#returns = sapply(y,expectedReturn)
```
```{r Q5-2 plot}
#plot(returns ~ y, type="l")
#abline(0,0)
```
III. Compute the optimal investment y∗.
$$y^* = argmax\ E[p(x,y)|y],\ y \in (0,1)$$
```{r Q5-3}
newton_optim = function(f, x0, tol=1e-10, n=1000, maximize=FALSE) {
  if(maximize) {
    f = function(x) {-1*f(x)}
  }
  history = x0
  
  for(i in 1:n) {
    deriv = genD(func=f, x=x0)$D
    x1 = x0 - deriv[1]/deriv[2]
    history = c(history, x1)
    if(abs(x1 - x0) < tol) {
      return(list(`optim x`=x1, `iterations`=i, `history`=history))
    }
    # Continue iteration
    x0 = x1
  }
  print("Maximum number of iteration reached")
  return(list(`optim x`=x1, `iterations`=i, `history`=history))
}

#optimize(function(y){-1*expectedReturn(y)},c(0,1))
# 0.4637619
#newton_optim(expectedReturn, 0)
# 0.4637619
```